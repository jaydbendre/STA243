{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Implement GMM clustering on the MNIST Dataset using spherical and diagonal Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required libraries\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "test_datapath = './Datasets/Test Images/t10k-images.idx3-ubyte'\n",
    "train_datapath = './Datasets/Train Images/train-images.idx3-ubyte'\n",
    "test_labelpath = './Datasets/Test Labels/t10k-labels.idx1-ubyte'\n",
    "train_labelpath = './Datasets/Train Labels/train-labels.idx1-ubyte'\n",
    "\n",
    "# Reading the u-byte files and converting them into numpy arrays\n",
    "X_train = idx2numpy.convert_from_file(train_datapath)\n",
    "y_train = idx2numpy.convert_from_file(train_labelpath)\n",
    "\n",
    "X_test = idx2numpy.convert_from_file(test_datapath)\n",
    "y_test = idx2numpy.convert_from_file(test_labelpath)\n",
    "\n",
    "# Filtering the dataset to only include the digits 0-4\n",
    "train_idx = np.where((y_train == 0) | (y_train == 1) | \n",
    "                     (y_train == 2) | (y_train == 3) | (y_train == 4))\n",
    "\n",
    "test_idx = np.where((y_test == 0) | (y_test == 1) | \n",
    "                     (y_test == 2) | (y_test == 3) | (y_test == 4))\n",
    "\n",
    "# Subsetting the dataset\n",
    "X_train = X_train[train_idx]\n",
    "X_test = X_test[test_idx]\n",
    "y_train = y_train[train_idx]\n",
    "y_test = y_test[test_idx]\n",
    "\n",
    "# Normalizing the pixel values for easier computation\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressing the images into 14x14 pixels\n",
    "X_train_compressed = np.zeros((X_train.shape[0], 14,14))\n",
    "X_test_compressed = np.zeros((X_test.shape[0], 14,14))\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_compressed[i] = np.add.reduceat(np.add.reduceat(X_train[i], np.arange(0,X_train[i].shape[0],2), axis=0)\n",
    "                                            , np.arange(0,X_train[i].shape[1],2), axis=1)/4\n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_compressed[i] = np.add.reduceat(np.add.reduceat(X_test[i], np.arange(0,X_test[i].shape[0],2), axis=0)\n",
    "                                            , np.arange(0,X_test[i].shape[1],2), axis=1)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the dataset to be of the form (n, 14*14)\n",
    "X_train_reshaped = X_train_compressed.reshape(X_train_compressed.shape[0], X_train_compressed.shape[1]*X_train_compressed.shape[2])\n",
    "X_test_reshaped = X_test_compressed.reshape(X_test_compressed.shape[0], X_test_compressed.shape[1]*X_test_compressed.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(k, cluster_size, spherical=False, init = True, mean = np.zeros(1), cov = np.zeros(1), seed  = np.random.randint(1,1000)):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    A custom function that generates a stack of multivariate normal distributions \n",
    "\n",
    "    Args:\n",
    "        k (int): Dimensions of the distributions to generate\n",
    "        cluster_size (int): Number of distributions to generate\n",
    "        spherical (bool, optional): Indicator for Spherical or Diagonal Gaussians. Defaults to False.\n",
    "        init (bool, optional): Indicator for whether its the first time the functions called or not. Defaults to True.\n",
    "        mean (list, optional): Mean vector if init = False. Defaults to np.zeros(1).\n",
    "        cov (list, optional): Cov vector if init = False. Defaults to np.zeros(1).\n",
    "        seed (int, optional): Random seed generator. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Stack of multivariate normal distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    # Check if its the first time the function is called\n",
    "    if init:\n",
    "        # Generating the mean vectors\n",
    "        means = np.array([np.random.rand(k) for _ in range(cluster_size)])\n",
    "        # Check if the distribution is spherical or diagonal\n",
    "        if spherical:\n",
    "            # Generating the covariance matrices  \n",
    "            covariance_matrix = [np.diag(np.ones(k)*np.random.rand()) for _ in range(cluster_size)]\n",
    "        else:\n",
    "            covariance_matrix = [np.diag(np.random.rand(k)) for _ in range(cluster_size)]\n",
    "        \n",
    "        # Creating the stack of multivariate normal distributions\n",
    "        normal_distributions = [multivariate_normal(\n",
    "            mean=means[i], cov=covariance_matrix[i]) for i in range(cluster_size)]\n",
    "    else:\n",
    "        # Creating the stack of multivariate normal distributions based on given values\n",
    "        normal_distributions = [multivariate_normal(mean = mean[i], cov = cov[i]) \n",
    "                                for i in range(cluster_size)]\n",
    "    \n",
    "    # Returning the stack of multivariate normal distributions\n",
    "    return np.array(normal_distributions)\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Q(theta^(t),theta)\n",
    "def log_likelihood(X, initial_distributions,pi, clusters):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    Function to generate values of Q(theta^(t),theta)\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        initial_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        clusters (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        np.array: Q(theta^(t),theta) values for every cluster\n",
    "    \"\"\"\n",
    "    values = np.array([\n",
    "        initial_distributions[i].logpdf(X) + np.log(pi[i]) for i in range(clusters)\n",
    "    ])\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_likelihood(X, initial_distributions,pi, clusters):\n",
    "    \"\"\"Summary\n",
    "    Calculates the log likelihood of the data given the initial distributions\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        initial_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        clusters (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        np.float64: Log likelihood of the data given the initial distributions\n",
    "    \"\"\"\n",
    "    values = np.sum([\n",
    "        initial_distributions[i].pdf(X)* pi[i] for i in range(clusters)\n",
    "    ])\n",
    "    \n",
    "    return np.log(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_step(X,initial_distributions, pi, k):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "\n",
    "    Function to emulate the expectation step of the EM algorithm\n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        initial_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        k (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        tuple: Stores the value of F and the cluster values for every x_i\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing the predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Calculating Q(theta^(t),theta)\n",
    "    log_likelihood_values = log_likelihood(X, initial_distributions, pi, k)\n",
    "    \n",
    "    # Initializing F \n",
    "    F = np.zeros_like(log_likelihood_values.T)\n",
    "    \n",
    "    # Parsing the log likelihood values \n",
    "    for i in range(log_likelihood_values.T.shape[0]):\n",
    "        # Calculating the value of F for a data point x_i \n",
    "        F[i,] = np.exp(log_likelihood_values[:,i]-logsumexp(log_likelihood_values[:,i]))\n",
    "        \n",
    "        # Seeing which cluster the data point belongs to\n",
    "        max_val = np.argmax(F[i,:])\n",
    "        \n",
    "        # Storing the cluster value for the data point\n",
    "        predictions.append(max_val)\n",
    "        \n",
    "    # Returning the value of F and the cluster values for every x_i\n",
    "    return (F, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization_step(X, F, clusters):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    \n",
    "    Function to emulate the maximization step of the EM algorithm\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Data Matrix\n",
    "        F (np.ndarray): Value of F for every data point obtained from the expectation step\n",
    "        clusters (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        tuple: vector of means and covariance matrices and updated probabilities for every cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the means and covariance matrices\n",
    "    means = []\n",
    "    covs = []\n",
    "    pis = []\n",
    "\n",
    "    # Parsing the F values for a fixed cluster\n",
    "    for j in range(clusters):\n",
    "        # Calculating the denominator of the mean and covariance matrix computation \n",
    "        denominator = np.sum(F[:,j], axis=0)\n",
    "        \n",
    "        # Calculating the mean for the cluster and storing it\n",
    "        means.append(np.sum(X * F[:,j].reshape(len(X),1), axis=0)/denominator)\n",
    "        \n",
    "        # Calculating the covariance diagonal for the cluster and storing it\n",
    "        cov_diag = []\n",
    "        for i in range(X.shape[1]):\n",
    "            # Creating the centering of the expression\n",
    "            center = ((X[:,j]-means[j][i])**2).reshape(-1,1)\n",
    "            # Calculating the covariance diagonal for the cluster and storing it\n",
    "            sigma = (1/denominator) * np.dot(F[:,j].reshape(1, len(X)), center)\n",
    "            \n",
    "            # Adding the diagonal to the list and adding 0.05 to prevent underflow\n",
    "            cov_diag.append(sigma[0][0] + 0.05)\n",
    "        \n",
    "        # Storing the covariance diagonal for the cluster\n",
    "        covs.append(np.diag(cov_diag))\n",
    "        \n",
    "        # Calculating the probability of the cluster and storing it\n",
    "        pis.append(denominator/np.sum(F))\n",
    "    \n",
    "    # Returning the vector of means and covariance matrices and updated probabilities for every cluster\n",
    "    return (means, covs, pis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_maximization(X, normal_distributions, pi, k, threshold = 0.0001, maxiters = 1000):\n",
    "    \"\"\"\n",
    "    Summary\n",
    "\n",
    "    Function to emulate the expectation maximization algorithm \n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Data Matrix\n",
    "        normal_distributions (list): Stack of multi-variate normal distributions\n",
    "        pi (list): Probability of each distribution\n",
    "        k (int): Number of clusters\n",
    "        threshold (float, optional): Threshold that reduces the effect of . Defaults to 0.0001.\n",
    "        maxiters (int, optional): Maximum iterations incase the difference of likelihood doesnt converge. Defaults to 1000.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: vector of predictions, iterations taken and trend of the differences of likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the difference of likelihood\n",
    "    diff = []\n",
    "    for i in range(maxiters):\n",
    "        # Calculating the value of log likelihood\n",
    "        # and the cluster values for every x_i with t^th iteration mean and covariance\n",
    "        t_1_ll = calc_log_likelihood(X, normal_distributions, pi, k)    \n",
    "        \n",
    "        # Running the expectation step\n",
    "        F,predictions = expectation_step(X, normal_distributions, pi, k)\n",
    "        # Running the maximization step\n",
    "        means, covs, pi = maximization_step(X, F, k)\n",
    "        \n",
    "        # Updating the normal distributions based on new values of means and covariance matrices \n",
    "        normal_distributions = generate_data(X.shape[1], k, mean = means, cov = covs, init = False)\n",
    "        \n",
    "        # Calculating the value of log likelihood\n",
    "        # and the cluster values for every x_i with t+1^th iteration mean and covariance\n",
    "        t_ll = calc_log_likelihood(X, normal_distributions, pi, k)\n",
    "        \n",
    "        # Calculating the difference of likelihood\n",
    "        t_diff = np.abs(t_ll - t_1_ll)\n",
    "        \n",
    "        # Storing the difference of likelihood\n",
    "        diff.append(t_diff)\n",
    "        \n",
    "        # Checking if the difference of likelihood is less than the threshold\n",
    "        if t_diff < threshold:\n",
    "            return (predictions,i,diff)\n",
    "    \n",
    "    # Returning the vector of predictions, iterations taken and trend of the differences of likelihood\n",
    "    return (predictions,maxiters,diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_train,predictions):\n",
    "    # Subsetting the data based on the predictions\n",
    "    idx_0 = np.where(np.array(predictions) == 0)\n",
    "    idx_1 = np.where(np.array(predictions) == 1)\n",
    "    idx_2 = np.where(np.array(predictions) == 2)\n",
    "    idx_3 = np.where(np.array(predictions) == 3)\n",
    "    idx_4 = np.where(np.array(predictions) == 4)\n",
    "\n",
    "    idxs = [idx_0, idx_1, idx_2, idx_3, idx_4]\n",
    "    label_map = dict()\n",
    "    for i,id in enumerate(idxs):\n",
    "        # Setting label to the cluster\n",
    "        label_map[i] = np.bincount(y_train[id]).argmax()\n",
    "\n",
    "    # Generating correct label\n",
    "    labels = [label_map[i] for i in predictions]\n",
    "\n",
    "    print(\"Accuracy: \", accuracy_score(y_train, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting intial parameters\n",
    "N = X_train.shape[0]\n",
    "k = 5 \n",
    "threshold = 0.0001 \n",
    "\n",
    "# Generating random probabilities adding to one\n",
    "pi = np.random.dirichlet(np.ones(k))\n",
    "\n",
    "# Generating the initial distributions for spherical gaussian\n",
    "spherical_gaussian_distributions_1 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = True, seed = 1)\n",
    "spherical_gaussian_distributions_2 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = True, seed = 2)\n",
    "spherical_gaussian_distributions_3 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = True, seed = 3)\n",
    "\n",
    "# Generating the initial distributions for diagonal gaussian\n",
    "diagonal_gaussian_distributions_1 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = False, seed = 1)\n",
    "diagonal_gaussian_distributions_2 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = False, seed = 2)\n",
    "diagonal_gaussian_distributions_3 = generate_data(X_train_reshaped.shape[1], k, init = True, spherical = False, seed = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7105177147339522\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for spherical gaussian\n",
    "predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         spherical_gaussian_distributions_1, pi, k, threshold = threshold)\n",
    "\n",
    "get_accuracy(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7105177147339522\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for spherical gaussian\n",
    "predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         spherical_gaussian_distributions_2, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7105177147339522\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for spherical gaussian\n",
    "predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         spherical_gaussian_distributions_3, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.844260687671591\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for diagonal gaussian\n",
    "diagonal_predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         diagonal_gaussian_distributions_1, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, diagonal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7106157667668976\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for diagonal gaussian\n",
    "diagonal_predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         diagonal_gaussian_distributions_2, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, diagonal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8090926918551444\n"
     ]
    }
   ],
   "source": [
    "# Running the expectation maximization algorithm for diagonal gaussian\n",
    "diagonal_predictions, iterations, diff = expectation_maximization(X_train_reshaped, \n",
    "                                                         diagonal_gaussian_distributions_3, pi, k, threshold = threshold)\n",
    "get_accuracy(y_train, diagonal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2db188c233a700111d30e3ccbeb8d5ce15dbfb83d194c6870e309733805c539"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
